{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ac812fb-aefb-4e46-8498-99a6f7829d06",
   "metadata": {},
   "source": [
    "# Sentiments On Crowd-Flower Brands and Products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258ca47d-64b3-4749-8336-9a70962894bd",
   "metadata": {},
   "source": [
    "# Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca2522c-15b4-408c-a4d4-6d241803d918",
   "metadata": {},
   "source": [
    "[1. Business Understanding](#1.-Business-Understanding) </br>\n",
    "[1.1 Business Description](#1.1-Business-Description) </br>\n",
    "[1.2 Problem Statement](#1.2-Problem-Statement) </br>\n",
    "[1.3 Main Objective](#1.3-Main-Objective) </br>\n",
    "[1.4 Specific Objectives](#1.4-Specific-Objectives) </br>\n",
    "[2. Importing Libraries And Warnings](#2.-Importing-Libraries-And-Warnings) </br>\n",
    "[3. Data Understanding](#3.-Data-Understanding) </br>\n",
    "[3.1 Information about the columns](#3.1-Information-About-The-Columns)</br>\n",
    "[4. Data Preparation](#4.-Data-Preparation) </br>\n",
    "[4.1 Visualizing Before Cleaning](#4.1-Visualizing-Before-Cleaning) </br>\n",
    "[4.2 Renaming The Columns](#4.2-Renaming-The-Columns) </br>\n",
    "[4.3 Missing Values](#4.3-Missing-Values) </br>\n",
    "[4.4 Duplicates](#4.4-Duplicates) </br>\n",
    "[4.5 Place Holders](#4.5-Place-Holders) </br>\n",
    "[4.6 White Space](#4.6-White-Space) </br>\n",
    "[4.7 Visualizing After Cleaning](#4.7-Visualizing-After-Cleaning) </br>\n",
    "[5. EDA](#5.-EDA) </br>\n",
    "[6. Data Preprocessing And Feature Engineering](#6.-Data-Preprocessing-And-Feature-Engineering) </br>\n",
    "[7. Data Modelling](#7.-Data-Modelling) </br>\n",
    "[8. Evaluation](#8.-Evaluation) </br>\n",
    "[9. Conclusions](#9.-Conclusions) </br>\n",
    "[10. Recommendations](#10.-Recommendations) </br>\n",
    "[11. Challenges](#11.-Challenges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11a0b71-8831-4ea9-8dc1-7c849c9f6f3c",
   "metadata": {},
   "source": [
    "# 1. Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8abdfa1-b6e9-48e3-a3a1-6b7871b69f01",
   "metadata": {},
   "source": [
    "## 1.1 Business Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15897e4d-a988-468a-9601-2ca4f602f027",
   "metadata": {},
   "source": [
    "In the competitive landscape of today's market, establishing a profound connection between products or services and the emotions of the target audience is a strategic imperative. Emotional branding offers the potential to cultivate brand loyalty, trust, and differentiation. </br>\n",
    "To harness the power of emotional branding, businesses must accurately gauge public sentiment towards their offerings, ensuring that the emotional connection aligns with their brand vision. By analyzing various sources of customer feedback, such as social media posts, sentiment analysis can determine whether the sentiment of the feedback is positive, negative, or neutral. </br>\n",
    "Employing sentiment analysis can lead to data-driven decisions, improving overall performance and reputation, and resulting in increased profitability and success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b046e8-e5b5-49b5-9e9c-f33a9e7b8a84",
   "metadata": {},
   "source": [
    "## 1.2 Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a8971c-1e56-48b9-abc2-73b0cef6519d",
   "metadata": {},
   "source": [
    "Emotional branding, a strategy centered on evoking specific emotions in customers, has become a critical driver of brand loyalty and differentiation. Establishing a deep emotional connection between brands and consumers is essential. </br>\n",
    "However, the challenge lies in accurately assessing and aligning a brand's emotional resonance with customer sentiment in real-time. To address this challenge, this project seeks to develop a sentiment analysis solution that deciphers the emotions expressed in tweets related to Apple and Google products. </br>\n",
    "The aim to equip businesses shareholders with actionable insights to strategically employ emotional branding and strengthen their brand-consumer relationships. The ultimate goal is to empower organizations to enhance customer loyalty, trust, and competitiveness by aligning their products with the emotions of their target audience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a07d5a-b258-4acc-acdf-94303b2506d5",
   "metadata": {},
   "source": [
    "## 1.3 Main Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302645cc-964d-4fca-8643-cb0b1d62c413",
   "metadata": {},
   "source": [
    "To use Natural Language processing to build models which can rate a sentiment on Google and Apple based on tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47db71af-ae63-4044-9e4e-6963506ae985",
   "metadata": {},
   "source": [
    "## 1.4 Specific Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc6891a-7691-4300-be06-f9f1e30e793f",
   "metadata": {},
   "source": [
    "a. To use both NLP and deep learning models to classify tweets into positive, neutral and negative.</br>\n",
    "b. To visualize sentiment analysis, gain insights from them hence evaluate model performances to conlude informed decisions and recommendations.</br>\n",
    "c. Understanding and preprocessing dataset of tweets related to google and apple products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0a4b7b-8dcf-4b44-ade3-b3aadb899067",
   "metadata": {},
   "source": [
    "# 2. Importing Libraries And Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60a90e8b-52b2-480b-9124-26361515fd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/muchiri/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'triu' from 'scipy.linalg' (/usr/local/lib/python3.10/dist-packages/scipy/linalg/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/__init__.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/corpora/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/corpora/indexedcorpus.py:14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[1;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/interfaces.py:19\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[1;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/matutils.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs, triu\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m psi  \u001b[38;5;66;03m# gamma function utils\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'triu' from 'scipy.linalg' (/usr/local/lib/python3.10/dist-packages/scipy/linalg/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "nltk.download('wordnet')\n",
    "from nltk import FreqDist\n",
    "from textblob import TextBlob\n",
    "from nltk import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from nltk.corpus import stopwords\n",
    "from keras.optimizers import Adam\n",
    "from gensim.models import Word2Vec\n",
    "from collections import defaultdict\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, models\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, Activation, Bidirectional, GlobalMaxPool1D, GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b221699-72d8-4617-8036-b565bfcf726d",
   "metadata": {},
   "source": [
    "# 3. Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc5d57b1-6d11-4970-990d-a75a49763469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>Ipad everywhere. #SXSW {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>Wave, buzz... RT @mention We interrupt your re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>Google's Zeiger, a physician never reported po...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>Some Verizon iPhone customers complained their...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9092</th>\n",
       "      <td>Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9093 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text  \\\n",
       "0     .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1     @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2     @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3     @sxsw I hope this year's festival isn't as cra...   \n",
       "4     @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "...                                                 ...   \n",
       "9088                      Ipad everywhere. #SXSW {link}   \n",
       "9089  Wave, buzz... RT @mention We interrupt your re...   \n",
       "9090  Google's Zeiger, a physician never reported po...   \n",
       "9091  Some Verizon iPhone customers complained their...   \n",
       "9092  Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at  \\\n",
       "0                             iPhone   \n",
       "1                 iPad or iPhone App   \n",
       "2                               iPad   \n",
       "3                 iPad or iPhone App   \n",
       "4                             Google   \n",
       "...                              ...   \n",
       "9088                            iPad   \n",
       "9089                             NaN   \n",
       "9090                             NaN   \n",
       "9091                             NaN   \n",
       "9092                             NaN   \n",
       "\n",
       "     is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                      Negative emotion  \n",
       "1                                      Positive emotion  \n",
       "2                                      Positive emotion  \n",
       "3                                      Negative emotion  \n",
       "4                                      Positive emotion  \n",
       "...                                                 ...  \n",
       "9088                                   Positive emotion  \n",
       "9089                 No emotion toward brand or product  \n",
       "9090                 No emotion toward brand or product  \n",
       "9091                 No emotion toward brand or product  \n",
       "9092                 No emotion toward brand or product  \n",
       "\n",
       "[9093 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the data\n",
    "df = pd.read_csv(\"Data/crowdflower-brands-and-product-emotions/original/judge-1377884607_tweet_product_company.csv\", encoding = \"latin1\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d50c8292-9c63-46f3-ba1a-c688328aa1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       .@wesley83 I have a 3G iPhone. After 3 hrs twe...\n",
       "1       @jessedee Know about @fludapp ? Awesome iPad/i...\n",
       "2       @swonderlin Can not wait for #iPad 2 also. The...\n",
       "3       @sxsw I hope this year's festival isn't as cra...\n",
       "4       @sxtxstate great stuff on Fri #SXSW: Marissa M...\n",
       "                              ...                        \n",
       "9088                        Ipad everywhere. #SXSW {link}\n",
       "9089    Wave, buzz... RT @mention We interrupt your re...\n",
       "9090    Google's Zeiger, a physician never reported po...\n",
       "9091    Some Verizon iPhone customers complained their...\n",
       "9092    Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...\n",
       "Name: tweet_text, Length: 9093, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tweet_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "382f732f-3fa5-416d-abbe-c935cda00be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_text', 'emotion_in_tweet_is_directed_at',\n",
       "       'is_there_an_emotion_directed_at_a_brand_or_product'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a48e0b8-ac4f-4e8a-8181-d82a98025b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['iPhone', 'iPad or iPhone App', 'iPad', 'Google', nan, 'Android',\n",
       "       'Apple', 'Android App', 'Other Google product or service',\n",
       "       'Other Apple product or service'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the unique values in the emotion_in_tweet_is_directed_at column\n",
    "df[\"emotion_in_tweet_is_directed_at\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86966323-66d0-454d-ad5e-536a90aae765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Negative emotion', 'Positive emotion',\n",
       "       'No emotion toward brand or product', \"I can't tell\"], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the uniques values in is_there_an_emotion_directed_at_a_brand_or_product column\n",
    "df[\"is_there_an_emotion_directed_at_a_brand_or_product\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9fe22-d6da-4cbc-9421-48f404d2b97c",
   "metadata": {},
   "source": [
    "## 3.1 Information About The Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a0baf3-ac16-402e-8b94-1f6c54ca3672",
   "metadata": {},
   "source": [
    "* tweet_text - It contains information about the text\n",
    "  \n",
    "* emotion_in_tweet_is_directed_at - It contains information about the brand </br>\n",
    "  i.e; </br>\n",
    "  Apple </br>\n",
    "  iPhone </br>\n",
    "  Android App </br>\n",
    "  Google Android </br>\n",
    "  iPad or iPhone App </br>\n",
    "  Other Apple product or service </br>\n",
    "  Other Google product or service </br>\n",
    "  \n",
    "* is_there_an_emotion_directed_at_a_brand_or_product (will be used as the target) - It contains information about the emotion towards a    given brand </br>\n",
    "  i.e; </br>\n",
    "  I can't tell </br>\n",
    "  Positive emotion </br>\n",
    "  Negative emotion </br>\n",
    "  No emotion toward brand or product </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3787da0-9a84-40ea-b927-18961ad9257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data types of the column values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac47d74-6016-48ac-ad9b-55714d05446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarily statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a197f23-591b-45ca-9b6a-a4e4c9090654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of our data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0625c6-e109-4722-8ca8-0d1c0cfbc330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the type of values \n",
    "df.value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e31803b-d6d1-4216-af30-ff3b7d45180d",
   "metadata": {},
   "source": [
    "# 4. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ff1a4c-3f4b-4a9b-bcd5-7856b9b3868e",
   "metadata": {},
   "source": [
    "## 4.1 Visualizing Before Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c29a5e3-60bc-4201-8b42-2a9ce10cd595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting tweet_text values into string to enable plotting of the word cloud\n",
    "df[\"tweet_text\"] = df[\"tweet_text\"].astype(str)\n",
    "\n",
    "# plot the figure size\n",
    "plt.figure(figsize = (10, 6))\n",
    "\n",
    "# generate a word cloud from the data\n",
    "word_cloud = WordCloud(width = 600, height = 400).generate(\"\".join(df[\"tweet_text\"]))\n",
    "\n",
    "# plotting\n",
    "plt.imshow(word_cloud, interpolation = \"bilinear\")\n",
    "plt.title(\"Word cloud visual before cleaning\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# displaying the word count\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dda134-aa22-408f-8f2f-9737aac04218",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "It seems some words do appear more frequent than others: </br>\n",
    "More frequently appearing words: </br>\n",
    "1. SXSW </br>\n",
    "2. mention </br>\n",
    "3. Google </br>\n",
    "Less frequently appearing words: </br>\n",
    "i. block </br>\n",
    "ii. facebook </br>\n",
    "iii. well </br>\n",
    "iv. let </br>\n",
    "v. come </br>\n",
    "\n",
    "### Disclaimer: Most less frequently appearing words are stop words which will be dealt with in a few </br>\n",
    "### N/B: Will see if there will be changes in the word cloud after data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e91c762-a9b8-4a2a-9183-5b934e8b0b75",
   "metadata": {},
   "source": [
    "## 4.2 Renaming The Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf5f7fa-ed79-4c42-953a-e523d7f7dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the columns \n",
    "\"\"\"\n",
    "The columns names are too wordy and complex. \n",
    "It's better to keep them short but sensible\n",
    "\"\"\"\n",
    "df.rename(columns = {\"tweet_text\" : \"tweet\" , \"emotion_in_tweet_is_directed_at\" : \"brand\", \\\n",
    "                     \"is_there_an_emotion_directed_at_a_brand_or_product\" : \"emotion\"}, inplace = True)\n",
    "\n",
    "# display the first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b250b2-47b2-48d5-82fd-dd8ded38d428",
   "metadata": {},
   "source": [
    "## 4.3 Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800ea463-3fb9-474b-a7ac-e69ed577f91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing values and their sum\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b6afb2-f593-46c2-a421-82906f1a35f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "brand column seems to have missing values. \n",
    "\n",
    "However, they can't be discarded simply because they are missing values.\n",
    "\n",
    "Some missing values have meanings. Others are related to other columns in a meaninful way. Others are too many such that they encompass\n",
    "almost the entire column.\n",
    "\n",
    "Best practice is to confirm what these missing values are to know how best to deal with them.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a531a6-bf8e-4fc0-bf37-1fbb4487bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking brand column missing values percentage\n",
    "# length of the brand column missing values\n",
    "brand_missing_len = df[\"brand\"].isna().sum()\n",
    "\n",
    "# length of the entire column\n",
    "brand_len = len(df[\"brand\"])\n",
    "\n",
    "# percentage of missing values in the column\n",
    "perc_brand = (brand_missing_len / brand_len) * 100\n",
    "\n",
    "# rounding of the percentage to the nearest 4 d.p\n",
    "rounded_perc_brand = round(perc_brand, 4)\n",
    "\n",
    "# showing the results\n",
    "print(\"percentage of missing values in brand column:\", rounded_perc_brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab351255-d3a7-4eff-be3e-09f9ccc76403",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "64% of missing values is a lot to be dropped. An investigation is required here!\n",
    "checking if the missing values are related in any way to the other columns.\n",
    "This will help in knowing what type of missing values they are and the appropriate step to take.\n",
    "\"\"\"\n",
    "\n",
    "# selecting rows which have missing values in brand column\n",
    "brand_missing = df[df[\"brand\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4121eb8e-cb26-41df-bc63-cc91b2f865e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting rows in tweet column which have nan values in brand columns\n",
    "print(brand_missing[\"tweet\"].value_counts(dropna = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268c590e-263e-46f8-94f9-62bf64e801b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting rows in emotion column which have nan values in brand columns\n",
    "print(brand_missing[\"emotion\"].value_counts(dropna = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f497c8d-4937-4697-a24c-bf34bf018023",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdde6472-30be-4c4a-a4cd-d49c97524100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The fact that rows having \"No emotion toward brand or product\" are the most common with nan values in the brand column makes \"sense\" \n",
    "because the column was supposed to show emotions. If there is none, it means the there is no product/brand there in the brand column.\n",
    "The best approach in this case would be to rebrand the nan values as \"no brand\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1062dd-744d-4a55-8eac-e982d89b95df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the missing values using \"no brand\"\n",
    "df[\"brand\"].fillna(\"no brand\", inplace = True)\n",
    "\n",
    "# checking the values of the brand column\n",
    "print(df[\"brand\"].value_counts(dropna = False))\n",
    "\n",
    "# spacing out things\n",
    "print(\" \")\n",
    "\n",
    "# checking if there are any missing values\n",
    "print(\"number of missing values:\", df[\"brand\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3279ed-ead5-4fa8-9c7a-4674f716ca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All good. No missing values in the columns.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867d153c-4dc7-468c-89ee-0a6d9349d1bf",
   "metadata": {},
   "source": [
    "## 4.4 Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8949d2c-2f98-4f05-9516-ae82d3ba602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for any duplicates\n",
    "df.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ad79f4-f7f1-45a2-83c6-cd7e6e0db6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1786289-2869-4365-865a-a9faad76ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The duplicates are quite small i.e 22\n",
    "They can be dropped\n",
    "\"\"\"\n",
    "df.drop_duplicates(inplace = True)\n",
    "\n",
    "# checking if there are any more duplicates\n",
    "print(\"Number of duplicates:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be042e3-a2e6-4c89-9c96-1d368faaf193",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All is good. No duplicates in the columns\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbe958b-d632-4bfd-9730-76a646b3ff1f",
   "metadata": {},
   "source": [
    "## 4.5 Place Holders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e648f01c-ff60-4c61-9c11-3e9bdb309bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for place holders\n",
    "for column in df.columns:\n",
    "    print(df[column].value_counts(dropna = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7776e6e8-6435-471e-a51c-7a9dfc0a8be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Only the tweet column seems to have place holders. i.e # ? / e.t.c\n",
    "A function can be written with findall() method to search for all placeholders and subsequently remove them.\n",
    "\"\"\"\n",
    "# function to search and remove placeholders in tweet column\n",
    "def tweet_text(text):\n",
    "    # creating a basic pattern to search through the file for any instances that match the place holders\n",
    "    pattern = r\"[^\\w\\s]\"\n",
    "\n",
    "    # compile the pattern\n",
    "    p = re.compile(pattern)\n",
    "\n",
    "    # use findall() method o look for any place holders\n",
    "    placeholders = re.findall(p, text)\n",
    "\n",
    "    # remove all placeholders by replacing them with and empty space\n",
    "    cleaned_text = re.sub(p, \" \", text)\n",
    "\n",
    "    # return the cleaned text without placeholders\n",
    "    return cleaned_text\n",
    "    \n",
    "# checking if there any placeholders anymore\n",
    "df[\"tweet\"] = df[\"tweet\"].apply(tweet_text)\n",
    "df[\"tweet\"].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bdedb8-a48f-41e7-9b49-829eda9e4393",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All is good. No placeholders in the columns\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5931c9cf-981e-468c-b7bc-46d192301a6d",
   "metadata": {},
   "source": [
    "## 4.6 White Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6878e5ac-4a6a-486c-9faa-da4bf3bcaaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for white space\n",
    "df_whitespace = [col for col in df.columns if col.strip() != col]\n",
    "print(\"Columns with whitespace:\", df_whitespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dab250-ec89-480e-879a-0628554854bb",
   "metadata": {},
   "source": [
    "## 4.7 Visualizing After Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cbdf75-fb17-4c2f-8225-4bd380a617a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tweet_text values into string to enable plotting of the word cloud\n",
    "df[\"tweet\"] = df[\"tweet\"].astype(str)\n",
    "\n",
    "# plot the figure size\n",
    "plt.figure(figsize = (10, 6))\n",
    "\n",
    "# generate a word cloud from the data\n",
    "word_cloud = WordCloud(width = 600, height = 400).generate(\"\".join(df[\"tweet\"]))\n",
    "\n",
    "# plot\n",
    "plt.imshow(word_cloud, interpolation = \"bilinear\")\n",
    "plt.title(\"Word cloud visual after cleaning\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# display the wordcloud\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a3a6f-8a17-4e59-8476-96a36f431196",
   "metadata": {},
   "source": [
    "### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a89bdf2-78bb-4a7a-8d9f-5490ead3c1ad",
   "metadata": {},
   "source": [
    "Some words are quite constitent </br>\n",
    "Most frequent words:\n",
    "1. sxsw\n",
    "2. link\n",
    "3. mention </br>\n",
    "Less frequent words: </br>\n",
    "i. free </br>\n",
    "ii. ipad </br>\n",
    "iii. app </br>\n",
    "</br>\n",
    "Data is clean ready for next steps i.e Eda..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c96a2fa-b656-432f-80df-a526bafac7f3",
   "metadata": {},
   "source": [
    "# 5. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b296a7-4bfe-4208-894b-1acba976572b",
   "metadata": {},
   "source": [
    "## 5.1 Univariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0480b19e-cac3-40fb-bb27-6a1be055a848",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visual representation of the number of tweets per brand\n",
    "\"\"\"\n",
    "# plot a figure\n",
    "plt.figure(figsize = (10, 6))\n",
    "\n",
    "# defining variables for the bar graph\n",
    "x = df[\"brand\"].value_counts()\n",
    "y = x.values\n",
    "\n",
    "# plot a bar graph\n",
    "plt.bar(x.index, y)\n",
    "plt.title(\"Number of tweets per brand\")\n",
    "plt.xlabel(\"brand\")\n",
    "plt.ylabel(\"tweets\")\n",
    "\n",
    "# rotating the x axis to enable readability\n",
    "plt.xticks(rotation = 90)\n",
    "\n",
    "# displaying the bar graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e44d88-00f4-4eec-a4b9-9fdfbe7c6550",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "visual representation of the number of tweets per emotion\n",
    "\"\"\"\n",
    "# plotting the figure size\n",
    "plt.figure(figsize = (10, 6))\n",
    "emotions_count = df[\"emotion\"].value_counts()\n",
    "\n",
    "# plotting the bar plot\n",
    "sns.barplot(x = emotions_count.index, y = emotions_count.values)\n",
    "\n",
    "# labelling the bar chart\n",
    "plt.title(\"Number of tweets per emotion\")\n",
    "plt.xlabel(\"emotion\")\n",
    "plt.ylabel(\"number of tweets\")\n",
    "\n",
    "# rotating the x=axis for readability\n",
    "plt.xticks(rotation = 45)\n",
    "\n",
    "# displaying the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a232b26-13f1-4e31-be14-4b63810d88a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "getting the 10 most common words\n",
    "\"\"\"\n",
    "\n",
    "# plotting a frequency distribution visual\n",
    "freqdist = FreqDist(df[\"tweet\"])\n",
    "most_common = freqdist.most_common(10)\n",
    "\n",
    "# display the results\n",
    "most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17a5469-eb06-4f1e-81cc-7bb484734119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe for the most common words\n",
    "most_common_df = pd.DataFrame(most_common, columns = [\"Word\", \"Frequency\"])\n",
    "\n",
    "# plotting the size \n",
    "plt.figure(figsize = (10, 6))\n",
    "\n",
    "# creating a bar graph\n",
    "most_common_df.plot(kind = \"bar\", x = \"Word\", y = \"Frequency\")\n",
    "\n",
    "# labelling the bar graph\n",
    "plt.title(\"10 most common words\")\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# rotating the x-axis for readbility\n",
    "plt.xticks(rotation = 90)\n",
    "\n",
    "# display the visual\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfb347b-c88b-4a72-b3a8-497ac874b0b9",
   "metadata": {},
   "source": [
    "## 5.2 Bivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da90fc7-c1c7-479f-8336-04d7a460ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visual representation of the emotions per brand\n",
    "\"\"\"\n",
    "# plot a figure\n",
    "plt.figure(figsize = (16, 12))\n",
    "x = df.groupby([\"brand\", \"emotion\"]).size().unstack(fill_value=0)\n",
    "\n",
    "# plot a stacked bar chart\n",
    "x.plot(kind = \"bar\", stacked = True)\n",
    "\n",
    "# labelling the bar chart\n",
    "plt.title(\"Distribution of brand per emotion\")\n",
    "plt.xlabel(\"brand\")\n",
    "plt.ylabel(\"emotion\")\n",
    "\n",
    "# rotating the x-axis the readability\n",
    "plt.xticks(rotation = 90)\n",
    "\n",
    "# display the bar chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a59a1b0-98b2-4fd5-97cb-19e99581175d",
   "metadata": {},
   "source": [
    "## 5. 3 Multivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11108f56-9753-4388-8547-7e97c2ae3501",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Looking for 10 common words for the positive and negative emotion.\n",
    "\n",
    "Plotting bar graphs o show how well they appear\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e86b5b-bffa-43bc-b841-42748ddcf84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nitializing dictionarieis to store word frequencies for each emotion\n",
    "emotion_word_frequencies = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# for loop for iterating through the rows of the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    emotion = row[\"emotion\"]\n",
    "    tweet_words = row[\"tweet\"].split() \n",
    "    \n",
    "    # for loop to update word frequencies\n",
    "    for word in tweet_words:\n",
    "        emotion_word_frequencies[emotion][word] += 1\n",
    "\n",
    "# defining variables for the emotions \n",
    "emotions_to_visualize = [\"Negative emotion\", \"Positive emotion\"]\n",
    "\n",
    "# defining a variable for the number of word\n",
    "num_top_words = 20\n",
    "\n",
    "# Creating bar graphs for each emotion\n",
    "for emotion in emotions_to_visualize:\n",
    "    word_frequencies = emotion_word_frequencies[emotion]\n",
    "    sorted_words = sorted(word_frequencies.items(), key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    # extracting the top words and their frequencies\n",
    "    top_words = [word for word, freq in sorted_words[:num_top_words]]\n",
    "    top_frequencies = [freq for word, freq in sorted_words[:num_top_words]]\n",
    "    \n",
    "    # creating a DataFrame for the most common words for the current emotion\n",
    "    most_common_df = pd.DataFrame({\"Word\": top_words, \"Frequency\": top_frequencies})\n",
    "    \n",
    "    # creating a bar graph\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    most_common_df.plot(kind = \"bar\", x = \"Word\", y = \"Frequency\", legend = False)\n",
    "    \n",
    "    # labeling the bar graph\n",
    "    plt.title(f\"Top {num_top_words} words for {emotion}\")\n",
    "    plt.xlabel(\"Word\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    \n",
    "    # rotate the x-axis labels for readability\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    # displaying the visual\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c43106-41f0-4ec3-8762-39f5e5b8ba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "As mentioned in the word word cloud, some words are more constitent and mentioned in borh emotions.\n",
    "\n",
    "It's no surprise that sxsw, mention and of course some stop words like the again, appear to be most common here too\n",
    "\n",
    "Speaking of stop words...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d87c92d-ea1b-4ae2-8d5f-3f347287287d",
   "metadata": {},
   "source": [
    "# 6. Data Preprocessing And Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71796d59-2e94-4de3-b661-c0417cfbf5f8",
   "metadata": {},
   "source": [
    "## 6.1 Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4b3b75-9f7e-41f2-8377-43fe3d47138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Words such as \"the, and, of, is etc in the data add little or no value to the data.\n",
    "\n",
    "Here, the data is tokenized to make it easier to remove the stop words\n",
    "\n",
    "A list comprehension will be used to omit any token not in stop_words variable below\n",
    "\"\"\"\n",
    "# getting ENGLish stop words and placing them in a variable\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# creating tokens in tweet column\n",
    "# using df[\"tweet\"] as the variable to assign on to avoid inconstitiences\n",
    "df[\"tweet\"] = df[\"tweet\"].apply(lambda X: word_tokenize(X))\n",
    "\n",
    "# lowercase all tokens\n",
    "df[\"tweet\"] = df[\"tweet\"].apply(lambda tokens: [w.lower() for w in tokens if w.lower() not in stop_words])\n",
    "\n",
    "# show the result\n",
    "print(df[\"tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8bf28d-3f62-4a45-a9bb-3caf1b8d7e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "As explected words such as \"the\", \"and\" and so on are nowhere to be seen\n",
    "\n",
    "Some words need to be reduced down to linguistically valid (root word).\n",
    "\n",
    "That's where Lemmatization comes in\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82563a7f-dad8-4a87-849f-1eb37e892edf",
   "metadata": {},
   "source": [
    "## 6.2 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83584265-791d-436e-b15c-15ebd9a09b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It's better than stemming since it word forms to determine the base or dictionary form rather than chopping off the last part of a sentence\n",
    "whether it's grammatically correct or not like stemming does\n",
    "\"\"\"\n",
    "# instantiating lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to lemmatize tweet column words\n",
    "def lemmatize_text(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# displaying the results\n",
    "df[\"tweet\"] = df[\"tweet\"].apply(lemmatize_text)\n",
    "df[\"tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1821cbf0-b82f-4d84-b1b6-3f9982cd0d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Great! Words such as customers have been reduced to customer indicating that lemmatization has taken place\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a8a0e8-e348-4652-9942-e20a026898e5",
   "metadata": {},
   "source": [
    "## 6.3 Padding and vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54a272a-7d07-4084-ac94-3ebdab2609b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Padding: ~ makes the sentences equal in length. They can be set to a specific length (100 in this case)\n",
    "\n",
    "Vectorizing: ~ converts text to numeric to be used for modelling such as neural networks.\n",
    "\"\"\"\n",
    "# converting the tweet column to a DataFrame\n",
    "df[\"tweet_numeric\"] = df[\"tweet\"].apply(lambda tokens: ' '.join(map(str, tokens)))\n",
    "\n",
    "# creating a vocabulary of unique tokens\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df[\"tweet_numeric\"])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# converting text to integer sequences\n",
    "sequences = tokenizer.texts_to_sequences(df[\"tweet_numeric\"])\n",
    "\n",
    "# padding sequences to the fixed length\n",
    "padded_sequences = pad_sequences(sequences, maxlen = 100, padding = \"post\", truncating = \"post\")\n",
    "\n",
    "# assigning the padded sequences back to the DataFrame\n",
    "df[\"tweet_numeric\"] = list(padded_sequences)\n",
    "\n",
    "# displaying the unique values\n",
    "unique_values = df[\"tweet_numeric\"].apply(tuple).unique()\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d737a4a-6cda-46bc-8a6a-8436596911eb",
   "metadata": {},
   "source": [
    "## 6.4 Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f174d46-9a56-4074-b1a4-edb465041ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Converting both the target and brand columns to numeric and assigning them their own columns\n",
    "\n",
    "They are both discrete\n",
    "\"\"\"\n",
    "\n",
    "# instializing the label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# the tweet column will be converted to numeric using word embedding in a moment\n",
    "columns_to_label_encode = df.drop(\"tweet\", axis = 1)\n",
    "\n",
    "# converting brand column to numeric\n",
    "df[\"brand_numeric\"] = label_encoder.fit_transform(df[\"brand\"])\n",
    "\n",
    "# mapping the labels\n",
    "brand_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "\n",
    "# converting emotion column to numeric\n",
    "df[\"emotion_numeric\"] = label_encoder.fit_transform(df[\"emotion\"])\n",
    "\n",
    "# mapping the labels\n",
    "emotion_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "\n",
    "# print the mapping\n",
    "print(brand_mapping)\n",
    "\n",
    "# space out the output\n",
    "print(\" \")\n",
    "\n",
    "# print the mapping\n",
    "print(emotion_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7957b8f7-8f66-4076-8a14-db8ab63e78a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking out the brand new numeric columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ff2124-afbb-49f3-a227-e81c8617d19b",
   "metadata": {},
   "source": [
    "## 6.5 Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c18ee8a-2c49-4164-9a28-5de7d725d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The number of instances belonging to a particular class may not be evenly distributed.\n",
    "\n",
    "One class may have significantly more values than other.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5ce5e5-a3b0-4e18-b407-8ead090bd45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for class imbalance in the target(emotion)\n",
    "print(df[\"emotion_numeric\"].value_counts())\n",
    "\n",
    "# space out\n",
    "print(\" \")\n",
    "\n",
    "print(emotion_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668e00ec-4c8d-418d-9b50-489f578dfc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "There is class imbalance between the first two \"No emotion toward brand or product\" and \"positive emotion\" and the last two \n",
    "i.e \"I can't tell\" and \"negative emotion\"\n",
    "\n",
    "Percentages can help to prove this further.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afc4aac-8993-4378-80d3-a6843e2ee9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of 0 and 1 vs 2 and 3\n",
    "# 0 and 1\n",
    "len_0 = len(df.loc[df[\"emotion_numeric\"] == 0]) \n",
    "len_1 = len(df.loc[df[\"emotion_numeric\"] == 1])\n",
    "len_0_and_1 = np.add(len_0, len_1)\n",
    "perc_len_0_and_1 = (len_0_and_1 / len(df[\"emotion_numeric\"])) * 100\n",
    "rounded_perc_len_0_and_1 = round(perc_len_0_and_1, 4)\n",
    "\n",
    "# 2 and 3\n",
    "len_2 = len(df.loc[df[\"emotion_numeric\"] == 2])\n",
    "len_3 = len(df.loc[df[\"emotion_numeric\"] == 3])\n",
    "len_2_and_3 = np.add(len_2, len_3)\n",
    "perc_len_2_and_3 = (len_2_and_3 / len(df[\"emotion_numeric\"])) * 100\n",
    "rounded_perc_len_2_and_3 = round(perc_len_2_and_3, 4)\n",
    "\n",
    "print(f\"The percentage of 0 and 1(I can't tell and negative emotion is):\", {rounded_perc_len_0_and_1}, \"%\", \"while the pecentage \\\n",
    "of 2 and 3 (No emotion toward brand or product and positive emotion is):\", {rounded_perc_len_2_and_3}, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6406265d-6fc0-4693-a3f3-c31e744a6deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modelling with such data can be problematic during model training and evaluation.\n",
    "\n",
    "It can lead to biased predictions and reduced performance. That's where SMOTE comes in.\n",
    "\n",
    "It generates new sample data by creating \"synthetic\" examples that are combinations of the closest minority class\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deedbc44-d115-438e-9750-c3b3b3523e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize smote\n",
    "smote = SMOTE()\n",
    "\n",
    "# features\n",
    "x = df[\"tweet_numeric\"].tolist()\n",
    "\n",
    "# target\n",
    "y = df[\"emotion_numeric\"]\n",
    "\n",
    "# split the data \n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# using smote to balance the data\n",
    "X_train_balance, y_train_balance = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# print the results\n",
    "print(pd.Series(y_train_balance).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d2f4ef-346a-4b3e-a176-670ed8b004bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now the classes in the target column have balanced.\n",
    "\n",
    "This are the classes which will be used for modelling\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd50bd31-ad8e-4bd5-9742-0dfeeed234d5",
   "metadata": {},
   "source": [
    "# 7. Data Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3738a44d-5e7a-4154-b5f0-d8f70e9d701d",
   "metadata": {},
   "source": [
    "## 7.1 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0512f0f4-b7f5-4544-a5ff-1a3b4931822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Long Short Term Memory Cells are specialized neurons for use in RNNs.\n",
    "\n",
    "In their arsenal is: input gate,\n",
    "                      forget gate,\n",
    "                      output gate.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f925b8cb-c5fc-4041-81ba-c1906fa74cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting X_train_balance and X_test from list to arrays\n",
    "X_train_balance = np.array(X_train_balance)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# ensuring that target labels are encoded for 4 classes to represent the 4 emotions\n",
    "y_train_balance_encoded = to_categorical(y_train_balance, num_classes = 4)\n",
    "y_test_encoded = to_categorical(y_test, num_classes = 4)\n",
    "\n",
    "# converting into to an array\n",
    "y_train_balance_encoded = np.array(y_train_balance_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1210cebd-3629-426e-b43e-5cdb9e4fd996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantianting the model \n",
    "lstm_model = Sequential()\n",
    "\n",
    "# constructing a neural network in embedding layer\n",
    "lstm_model.add(Embedding(input_dim = 95000, output_dim = 64))\n",
    "\n",
    "# lstm layer, dense layer, output layer\n",
    "lstm_model.add(LSTM(128, activation = \"relu\"))\n",
    "lstm_model.add(Dense(64, activation = \"relu\"))\n",
    "lstm_model.add(Dense(4, activation = \"softmax\"))\n",
    "\n",
    "# compiling the model\n",
    "lstm_model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "lstm_history = lstm_model.fit(X_train_balance, y_train_balance_encoded, epochs = 10, batch_size = 64, \\\n",
    "                              validation_data = (X_test, y_test_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78731cc6-f19e-4ae1-8b42-61b6b0ac6402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the loss and accuracy history\n",
    "loss = lstm_history.history[\"loss\"]\n",
    "accuracy = lstm_history.history[\"accuracy\"]\n",
    "\n",
    "# access validation loss and accuracy\n",
    "val_loss = lstm_history.history.get(\"val_loss\")\n",
    "val_accuracy = lstm_history.history.get(\"val_accuracy\")\n",
    "\n",
    "# creating a list of epochs for the x-axis\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# plotting the figure size\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "# plotting training loss\n",
    "plt.plot(epochs, loss, \"r\", label = \"Training Loss\")\n",
    "if val_loss:\n",
    "    plt.plot(epochs, val_loss, \"b\", label = \"Validation Loss\")\n",
    "\n",
    "# labelling the plot\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# plotting the figure size\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "# plotting training accuracy\n",
    "plt.plot(epochs, accuracy, \"r\", label = \"Training Accuracy\")\n",
    "if val_accuracy:\n",
    "    plt.plot(epochs, val_accuracy, \"b\", label = \"Validation Accuracy\")\n",
    "\n",
    "# labelling the plots\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "# displaying the visuals\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e5d2bf-123c-4746-8c66-69cdab3f01bd",
   "metadata": {},
   "source": [
    "## 7.2 GRUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5473a7e2-f2cb-4bd5-8ffb-90d901f7ab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(Gated Recurrent Units) GRUs are a \"modified\" version of LSTM. It passes along it's internal state at each time step.\n",
    "\n",
    "Unlike LSTMs, GRUs have two gates: Reset gate \n",
    "                                   Update gate\n",
    "                                 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384978a6-670c-4810-8990-56d4ca8b67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Procedure will still be the same as LSTM only that GRU method will be used here\n",
    "\"\"\"\n",
    "# instantianting the model \n",
    "gru_model = Sequential()\n",
    "\n",
    "# constructing a neural network in embedding layer\n",
    "gru_model.add(Embedding(input_dim = 95000, output_dim = 64))\n",
    "\n",
    "# lstm layer, dense layer, output layer\n",
    "gru_model.add(GRU(128, activation = \"relu\"))  \n",
    "gru_model.add(Dense(64, activation =\"relu\"))\n",
    "gru_model.add(Dense(4, activation = \"softmax\"))\n",
    "\n",
    "# Compiling the model\n",
    "gru_model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# training the model\n",
    "gru_history = gru_model.fit(X_train_balance, y_train_balance_encoded, epochs = 10, batch_size = 64, \\\n",
    "                            validation_data=(X_test, y_test_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84753d7b-b9e2-4488-82eb-f2914802b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the loss and accuracy history\n",
    "loss = gru_history.history[\"loss\"]\n",
    "accuracy = gru_history.history[\"accuracy\"]\n",
    "\n",
    "# access validation loss and accuracy \n",
    "val_loss = gru_history.history.get(\"val_loss\")\n",
    "val_accuracy = gru_history.history.get(\"val_accuracy\")\n",
    "\n",
    "# creating a list of epochs for the x-axis\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# plotting training loss\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss, \"r\", label = \"Training Loss\")\n",
    "if val_loss:\n",
    "    plt.plot(epochs, val_loss, \"b\", label = \"Validation Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, accuracy, \"r\", label = \"Training Accuracy\")\n",
    "if val_accuracy:\n",
    "    plt.plot(epochs, val_accuracy, \"b\", label = \"Validation Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5db852-a375-4a96-a4e6-b91e9cd5cdd9",
   "metadata": {},
   "source": [
    "## 7.3 Textblob (Polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcef804-97a0-4539-8daa-360412e73609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the tweet column to strings\n",
    "df[\"tweet\"] = df[\"tweet\"].astype(str)\n",
    "\n",
    "# >1 positive, <1 negative and 1 for neutral\n",
    "df[\"polarity\"] = df[\"tweet\"].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "df[\"sentiment_textblob\"] = df[\"polarity\"].apply(lambda x: \"positive\" if x > 0 else (\"negative\" if x < 0 else \"neutral\"))\n",
    "\n",
    "# display the results\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042ce73f-16f3-4af7-ab87-b35001c2837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the figure size\n",
    "plt.subplots(figsize= (11, 5))\n",
    "\n",
    "# plotting the bar plot\n",
    "df[\"sentiment_textblob\"].value_counts().plot(kind = \"bar\")\n",
    "\n",
    "# labelling the plot\n",
    "plt.title(\"Sentiment Analysis\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Counts\")\n",
    "\n",
    "# Show the value counts\n",
    "print(df[\"sentiment_textblob\"].value_counts())\n",
    "\n",
    "# display the visual\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26369296-8213-4045-93f4-7267c2399196",
   "metadata": {},
   "source": [
    "## 7.4 Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc0290-6f9a-49d5-b225-c4fb5abc3cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the model\n",
    "model = models.Sequential()\n",
    "\n",
    "# adding input layer\n",
    "model.add(layers.Dense(units = 100, activation = \"relu\", input_shape = (100,)))\n",
    "\n",
    "# adding hidden layers\n",
    "model.add(layers.Dense(units = 64, activation = \"relu\"))\n",
    "model.add(layers.Dense(units = 32, activation = \"relu\"))\n",
    "\n",
    "# adding output layer with appropriate units and activation function\n",
    "model.add(layers.Dense(units = 4, activation = \"softmax\"))\n",
    "\n",
    "# compiling the model\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate = 0.001), loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "# fitting the model\n",
    "model.fit(X_train_balance, y_train_balance_encoded, batch_size=32, epochs=20)\n",
    "\n",
    "# summary of the model\n",
    "model.summary()\n",
    "\n",
    "# training the model\n",
    "history = model.fit(X_train_balance, y_train_balance_encoded, epochs = 20, batch_size = 32)\n",
    "\n",
    "# displaying training loss\n",
    "train_loss = history.history[\"loss\"]\n",
    "epochs = range(1, len(train_loss) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05f4f0a-ef34-4533-b89f-21a90d918b38",
   "metadata": {},
   "source": [
    "# 8. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcc4d7a-9afe-4042-9ccd-d5cad152b86e",
   "metadata": {},
   "source": [
    "Although the most frequent words for both positive and negative are focused on technology (sxsw), products (apple, iphone), and social media activities (mention, rt(retweet)), their is some differences. </br>\n",
    "</br>\n",
    "Frequent positive words (which are not in the list of negative words): great and good shows satisfactory and positive sentiments. Frequent negative words (which are not in the list of positive words): quot (argument) and think shows unsatisfactory. </br>\n",
    "</br>\n",
    "But the BIGGEST INDICATOR here is that there are 4457 positive tweets and 815 tweets. This shows that most tweets expressed favour or satisfactory in the products. Negative ones although at the minority shows that some twitter users were critical about the products. </br>\n",
    "</br>\n",
    "An accuracy of 70% (Neural network) shows 70% of the models predictions were made correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc9145f-f9ce-4533-a756-11c0b21a730d",
   "metadata": {},
   "source": [
    "# 9. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c378c2-9979-4344-8c4c-4680f663a4a9",
   "metadata": {},
   "source": [
    "Celebrate Positive Interactions: Positive sentiment tweets often mention words like great and good. Brands should acknowledge and celebrate these positive interactions. Sharing such experiences can build brand loyalty and trust. </br>\n",
    "</br>\n",
    "Improve Communication: The analysis reveals that clear and effective communication is essential. Customers often express negative sentiments when they feel uninformed or frustrated. Brands should focus on enhancing their communication strategies. </br>\n",
    "</br>\n",
    "Enhance Customer Service: in the CrowdFlower brands and products sentiment analysis, customer service plays a pivotal role. Addressing customer service concerns mentioned in tweets can significantly improve overall sentiment. Prompt and helpful responses can turn negative sentiment into positive experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d0e6c8-9455-4d17-a817-ec5e03de269b",
   "metadata": {},
   "source": [
    "# 10. Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4eff1e-0d64-421b-8a81-6e611bb25b55",
   "metadata": {},
   "source": [
    "The stakeholders should: </br>\n",
    "1. Leverage sentiment analysis models. They should improve the areas which lead to negative sentiments by identifying specific issues mentioned in the negative sentiment tweets and priotize their resolution.\n",
    "2. Uphold positive ones. They should identify the positive tweets and feedback and capitalize on them hence use them to make progress.\n",
    "3. Brand promotion. Share positive sentiment and customer success stories on twitter and marketing channels to bolster brand awareness and trust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3919cd-d983-4ac6-95a0-0802773f9113",
   "metadata": {},
   "source": [
    "# 11. Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff898c4-90ae-411a-a5e9-bb120d9f707e",
   "metadata": {},
   "source": [
    "a. Data is way limited. Datasets should be diverse. </br>\n",
    "b. Quite a challenge developing NLP Models that accuately align tweets with specific emotions. </br>\n",
    "c. Scaling the solution for sentiment analysis and still maintain brand alignment with evolving emotions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
